{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# default_exp weights\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# from nbdev.export2html import nbdev_build_docs\n",
    "# nbdev_build_docs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights\n",
    "> Load weighted data, combine with photon data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os,  pickle, healpy\n",
    "import numpy as np\n",
    "from light_curves.config import Config, Files, PointSource\n",
    "from light_curves.load_gti import get_gti\n",
    "from light_curves.photon_data import get_photon_data\n",
    "\n",
    "config = None\n",
    "files = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up photon data for tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading photon data for using source Geminga to test adding weights\n",
      "Processing 11 FITS files with GTI information ...  11 files, 63635 intervals with 3,322 days live time\n",
      "\tGTI MJD range: 54682.66-58698.08, good fraction 0.83 \n",
      "Loading  132 months from Arrow dataset /home/burnett/data/dataset\n",
      "....................................................................................................................................\n",
      "\tSelected 1313726 photons within 5 deg of  (195.13,4.27)\n",
      "\tEnergies: 100.0-1000000 MeV\n",
      "\tDates:    2008-08-04 15:46 - 2019-08-03 01:17\n",
      "\tMJD  :    54682.7          - 58698.1         \n"
     ]
    }
   ],
   "source": [
    "config = Config() #mjd_range=(54682,55000 ))\n",
    "files = Files()\n",
    "source = PointSource('Geminga')\n",
    "\n",
    "if files.valid:\n",
    "    print(f'Loading photon data for using source {source.name} to test adding weights')\n",
    "    photon_data = get_photon_data(config, files,  source )\n",
    "else:\n",
    "    print('Not testing since no files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _load_weights(filename, ):\n",
    "    \"\"\"Load the weight informaton\n",
    "\n",
    "    filename: pickled dict with map info\n",
    "\n",
    "    \"\"\"\n",
    "    # load a pickle containing weights, generated by pointlike\n",
    "    assert os.path.exists(filename),f'File {filename} not found.'\n",
    "    with open(filename, 'rb') as file:\n",
    "        wtd = pickle.load(file, encoding='latin1')\n",
    "    assert type(wtd)==dict, 'Expect a dictionary'\n",
    "    test_elements = 'energy_bins pixels weights nside model_name radius order roi_name'.split()\n",
    "    assert np.all([x in wtd.keys() for x in test_elements]),f'Dict missing one of the keys {test_elements}'\n",
    "    if config.verbose>0:\n",
    "        print(f'Load weights from file {os.path.realpath(filename)}')\n",
    "        pos = wtd['source_lb']\n",
    "        print(f'\\tFound: {wtd[\"source_name\"]} at ({pos[0]:.2f}, {pos[1]:.2f})')\n",
    "    # extract pixel ids and nside used\n",
    "    wt_pix   = wtd['pixels']\n",
    "    nside_wt = wtd['nside']\n",
    "\n",
    "    # merge the weights into a table, with default nans\n",
    "    # indexing is band id rows by weight pixel columns\n",
    "    # append one empty column for photons not in a weight pixel\n",
    "    # calculated weights are in a dict with band id keys\n",
    "    wts = np.full((32, len(wt_pix)+1), np.nan, dtype=np.float32)\n",
    "    weight_dict = wtd['weights']\n",
    "    for k in weight_dict.keys():\n",
    "        t = weight_dict[k]\n",
    "        if len(t.shape)==2:\n",
    "            t = t.T[0] #???\n",
    "        wts[k,:-1] = t\n",
    "    return wts , wt_pix , nside_wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weights from file /mnt/c/users/thbur/OneDrive/fermi/weight_files/Geminga_weights.pkl\n",
      "\tFound: PSR J0633+1746 at (195.14, 4.27)\n"
     ]
    }
   ],
   "source": [
    "if files.valid:\n",
    "    weight_file =  os.path.join(files.weights, source.name+'_weights.pkl')\n",
    "    assert os.path.exists( weight_file ), 'Failed check for weights'\n",
    "    wts, wt_pix, nside_wt = _load_weights(weight_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _add_weights(wts, wt_pix, nside_wt, photon_data):\n",
    "    # get the photon pixel ids, convert to NEST (if not already) and right shift them\n",
    "\n",
    "    if not config.nest:\n",
    "        # data are RING\n",
    "        photon_pix = healpy.ring2nest(config.nside, photon_data.pixel.values)\n",
    "    else:\n",
    "        photon_pix = photon_data.pixel.values\n",
    "    to_shift = 2*int(np.log2(config.nside/nside_wt));\n",
    "    shifted_pix =   np.right_shift(photon_pix, to_shift)\n",
    "    bad = np.logical_not(np.isin(shifted_pix, wt_pix))\n",
    "    if config.verbose>0:\n",
    "        print(f'\\tApplyng weights: {sum(bad)} / {len(bad)} photon pixels are outside weight region')\n",
    "    if sum(bad)==len(bad):\n",
    "        a = np.array(healpy.pix2ang(nside_wt, wt_pix, nest=True, lonlat=True)).mean(axis=1).round(1)\n",
    "        b = np.array(healpy.pix2ang(nside_wt, shifted_pix, nest=True, lonlat=True)).mean(axis=1).round(1)\n",
    "\n",
    "        raise Exception(f'There was no overlap of the photon data at {b} and the weights at {a}')\n",
    "    shifted_pix[bad] = 12*nside_wt**2 # set index to be beyond pixel indices\n",
    "\n",
    "    # find indices with search and add a \"weights\" column\n",
    "    # (expect that wt_pix are NEST ordering and sorted)\n",
    "    weight_index = np.searchsorted(wt_pix,shifted_pix)\n",
    "    band_index = np.fmin(31, photon_data.band.values) #all above 1 TeV into last bin\n",
    "\n",
    "    # final grand lookup -- isn't numpy wonderful!\n",
    "    photon_data.loc[:,'weight'] = wts[tuple([band_index, weight_index])]\n",
    "    if config.verbose>0:\n",
    "        print(f'\\t{sum(np.isnan(photon_data.weight.values))} weights set to NaN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each photon, add a weight appropriate for its energy and position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tApplyng weights: 240 / 1313726 photon pixels are outside weight region\n",
      "\t233109 weights set to NaN\n"
     ]
    }
   ],
   "source": [
    "if files.valid:\n",
    "    _add_weights(wts, wt_pix, nside_wt , photon_data)\n",
    "    photon_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The photon data now has a weight columnn: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def add_weights(xconfig, xfiles, photon_data, source):\n",
    "    \"\"\" add weights for the soruce to the photons data\n",
    "    \"\"\"\n",
    "    global config, files\n",
    "    config = xconfig\n",
    "    files = xfiles\n",
    "\n",
    "    weight_file =  os.path.join(files.weights, source.name+'_weights.pkl')\n",
    "    assert os.path.exists(weight_file)\n",
    "\n",
    "    wts, wt_pix, nside_wt = _load_weights(weight_file)\n",
    "    _add_weights(wts, wt_pix, nside_wt, photon_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_weights(config, files, photon_data, source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_config.ipynb.\n",
      "Converted 01_effective_area.ipynb.\n",
      "Converted 02_load_gti.ipynb.\n",
      "Converted 03_exposure.ipynb.\n",
      "Converted 04_photon_data.ipynb.\n",
      "Converted 05_weights.ipynb.\n",
      "Converted 07_cells.ipynb.\n",
      "Converted 09_poisson.ipynb.\n",
      "Converted 10_loglike.ipynb.\n",
      "Converted 11_lightcurve.ipynb.\n",
      "Converted index.ipynb.\n",
      "Mon Dec  7 06:39:17 PST 2020\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()\n",
    "!date"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
