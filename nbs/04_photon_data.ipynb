{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# default_exp photon_data\n",
    "sys.path[0],_=os.path.split(sys.path[0])\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Photon Data\n",
    "\n",
    "> Load timed photon data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarks on loading the photon data around a source, subject to a GTI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import healpy\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from light_curves.config import MJD, UTC\n",
    "from light_curves.load_gti import get_gti\n",
    "\n",
    "def get_photon_data(config: 'configuration data', \n",
    "                    files:  'file locations',\n",
    "                    source: 'Source data',\n",
    "                    gti:   'Good Time Interval'=None, \n",
    "                    nest=True): \n",
    "    \"\"\"\n",
    "    Read photon data from a Parquet dataset, select cone around the\n",
    "    source, use exposure to add exposures, return DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # check GTI\n",
    "    if gti is None:\n",
    "        gti = get_gti(config, files.gti)\n",
    "\n",
    "    # cone geometry stuff: get corresponding pixels and center vector\n",
    "    l,b,radius = source.l, source.b, config.radius  \n",
    "    cart = lambda l,b: healpy.dir2vec(l,b, lonlat=True) \n",
    "    conepix = healpy.query_disc(config.nside, cart(l,b), np.radians(radius), nest=nest)\n",
    "    center = healpy.dir2vec(l,b, lonlat=True)\n",
    "\n",
    "    ebins = config.energy_edges\n",
    "    ecenters = np.sqrt(ebins[:-1]*ebins[1:]); \n",
    "    band_limits = 2*np.searchsorted(ecenters, config.energy_range) if config.energy_range is not None else None\n",
    "\n",
    "    def load_photon_data(table, tstart):\n",
    "        \"\"\"For a given month table, select photons in cone, add tstart to times, \n",
    "        return DataFrame with band, time, pixel, radius\n",
    "        \"\"\"\n",
    "        allpix = np.array(table.column('nest_index'))\n",
    "\n",
    "        def cone_select(allpix, conepix, shift=None):\n",
    "            \"\"\"Fast cone selection using NEST and shift\n",
    "            \"\"\"\n",
    "            if shift is None:\n",
    "                return np.isin(allpix, conepix)\n",
    "            assert nest, 'Expect pixels to use NEST indexing'\n",
    "            a = np.right_shift(allpix, shift)\n",
    "            c = np.unique(np.right_shift(conepix, shift))\n",
    "            return np.isin(a,c)\n",
    "\n",
    "        # a selection of all those in an outer cone\n",
    "        incone = cone_select(allpix, conepix, 13)\n",
    "\n",
    "        # times: convert to double, add to start, convert to MJD \n",
    "        time = MJD(np.array(table['time'],float)[incone]+tstart)\n",
    "        in_gti = gti(time)\n",
    "        if np.sum(in_gti)==0:\n",
    "            print(f'WARNING: no photons in GTI for month {month}!', out=sys.stderr)\n",
    "\n",
    "        pixincone = allpix[incone][in_gti]\n",
    "\n",
    "        # distance from center for all accepted photons\n",
    "        ll,bb = healpy.pix2ang(config.nside, pixincone,  nest=nest, lonlat=True)\n",
    "        cart = lambda l,b: healpy.dir2vec(l,b, lonlat=True) \n",
    "        t2 = np.degrees(np.array(np.sqrt((1.-np.dot(center, cart(ll,bb)))*2), np.float32)) \n",
    "\n",
    "        # assemble the DataFrame, remove those outside the radius\n",
    "        out_df = pd.DataFrame(np.rec.fromarrays(\n",
    "            [np.array(table['band'])[incone][in_gti], time[in_gti], pixincone, t2], \n",
    "            names='band time pixel radius'.split()))\n",
    "\n",
    "        # apply final selection for radius and energy range\n",
    "\n",
    "        if band_limits is None: return out_df.query(f'radius<{radius}')\n",
    "\n",
    "        return out_df.query(f'radius<{radius} & {band_limits[0]} < band < {band_limits[1]}')\n",
    "\n",
    "    # get the monthly-partitioned dataset and tstart values\n",
    "    dataset = files.data+'/dataset'\n",
    "    tstart_dict= pickle.load(open(files.data+'/tstart.pkl', 'rb'))\n",
    "    months = tstart_dict.keys() \n",
    "\n",
    "    if config.verbose>0: \n",
    "        print(f'Loading  {len(months)} months from Arrow dataset {dataset}\\n', end='')\n",
    "\n",
    "    dflist=[] \n",
    "    for month, tstart in tstart_dict.items(): #months:\n",
    "        table= pq.read_table(dataset, filters=[f'month == {month}'.split()])\n",
    "        #tstart = tstart_dict[month]\n",
    "        d = load_photon_data(table, tstart)\n",
    "        if d is not None:\n",
    "            dflist.append(d)\n",
    "            if config.verbose>1: print('.', end='')\n",
    "        else:\n",
    "            if config.verbose>1: print('x', end='')\n",
    "            continue\n",
    "\n",
    "    assert len(dflist)>0, '\\nNo photon data found?'\n",
    "    df = pd.concat(dflist, ignore_index=True)\n",
    "    if config.verbose>0:\n",
    "        emin,emax = config.energy_range or (config.energy_edges[0],config.energy_edges[-1])\n",
    "        print(f'\\n\\tSelected {len(df)} photons within {config.radius}'\\\n",
    "              f' deg of  ({source.l:.2f},{source.b:.2f})')\n",
    "        print(f'\\tEnergies: {emin:.1f}-{emax:.0f} MeV')\n",
    "        ta,tb = df.iloc[0].time, df.iloc[-1].time\n",
    "        print(f'\\tDates:    {UTC(ta):16} - {UTC(tb)}'\n",
    "            f'\\n\\tMJD  :    {ta:<16.1f} - {tb:<16.1f}')  \n",
    "    return df  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loading a photon data set, for source Geminga\n",
      "Processing 11 GTI files ...  11 files, 63635 intervals with 3,322 days live time\n",
      "\tGTI MJD range: 54682.66-58698.08, good fraction 0.83 \n",
      "Loading data from 132 months from Arrow dataset /home/burnett/data/dataset\n",
      "....................................................................................................................................\n",
      "\tSelected 1313726 photons within 5 deg of  (195.13,4.27)\n",
      "\tEnergies: 100.0-1000000 MeV\n",
      "\tDates:    2008-08-04 15:46 - 2019-08-03 01:17\n",
      "\tMJD  :    54682.7          - 58698.1         \n"
     ]
    }
   ],
   "source": [
    "from light_curves.config import Config, FileConfiguration, PointSource\n",
    "\n",
    "config = Config()\n",
    "files = FileConfiguration()\n",
    "source = PointSource('Geminga')\n",
    "if os.path.exists(files.aeff):\n",
    "    print(f'Test loading a photon data set, for source {source.name}')\n",
    "    photon_data = get_photon_data(config, files, source )\n",
    "else:\n",
    "    print('Not testing since no files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_config.ipynb.\n",
      "Converted 01_effective_area.ipynb.\n",
      "Converted 02_load_gti.ipynb.\n",
      "Converted 03_exposure.ipynb.\n",
      "Converted 04_photon_data.ipynb.\n",
      "Converted 05_weights.ipynb.\n",
      "Converted 07_binner.ipynb.\n",
      "Converted 09_poisson.ipynb.\n",
      "Converted Untitled.ipynb.\n",
      "Converted index.ipynb.\n",
      "Sat Dec  5 06:32:17 PST 2020\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()\n",
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
